{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (MNIST)\n",
    "1. Library, Data Preprocessing \n",
    "2. Solve classification with Tensorflow\n",
    "3. Show the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 17:05:14.147043 14052 deprecation.py:323] From <ipython-input-3-9d55ea7d2755>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0824 17:05:14.149037 14052 deprecation.py:323] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0824 17:05:14.152027 14052 deprecation.py:323] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0824 17:05:16.122473 14052 deprecation.py:323] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 17:05:16.614110 14052 deprecation.py:323] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0824 17:05:16.617091 14052 deprecation.py:323] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 17:05:17.378952 14052 deprecation.py:323] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "# a tuple of two arrays\n",
    "# image : batch size x 784\n",
    "# 1. batch size MNIST images\n",
    "# 2. batch size MNIST labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypter parameter setting\n",
    "\n",
    "learning_rate = 1e-4\n",
    "training_epochs = 10\n",
    "display_step = 1\n",
    "batch_size = 32\n",
    "activation = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "tf_X = tf.placeholder(\"float\", shape=[None, 784])\n",
    "tf_X_reshape = tf.reshape(tf_X, [-1,28,28,1])\n",
    "tf_Y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(tf_X_reshape, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "pred=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "loss = -tf.reduce_sum(tf_Y*tf.log(pred))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(tf_Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9604\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    sess.run(optimizer,feed_dict={tf_X: batch[0], tf_Y: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"% sess.run(accuracy, feed_dict={\n",
    "       tf_X: mnist.test.images, tf_Y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (CIFAR10)\n",
    "1. Library, Data Preprocessing \n",
    "2. Solve classification with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets.cifar10 import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 78s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# one-hot encoding\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypter parameter setting\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 100\n",
    "display_step = 10\n",
    "batch_size = 128\n",
    "activation = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 17:08:51.550672 14052 deprecation.py:323] From <ipython-input-11-e3a634b8c067>:52: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W0824 17:08:51.670352 14052 deprecation.py:506] From c:\\users\\sweet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "\n",
    "def CNN_model(x):\n",
    "    x_image = x\n",
    "    \n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 64], stddev=5e-2))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "    b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "\n",
    "    W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    b_conv4 = tf.Variable(tf.constant(0.1, shape=[128])) \n",
    "    h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4)\n",
    "\n",
    "    W_conv5 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    b_conv5 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5)\n",
    "\n",
    "\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 128, 384], stddev=5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "\n",
    "    h_conv5_flat = tf.reshape(h_conv5, [-1, 8*8*128])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "tf_X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "tf_Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "pred, logit = CNN_model(tf_X)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=logit))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(tf_Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    for i in range(10000):\n",
    "        batch = next_batch(128, x_train, y_train_one_hot.eval())\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={tf_X: batch[0], tf_Y: batch[1], keep_prob: 1.0})\n",
    "            loss_print = loss.eval(feed_dict={tf_X: batch[0], tf_Y: batch[1], keep_prob: 1.0})\n",
    "\n",
    "            print(\"iteration: %d, accuracy: %f, loss: %f\" % (i, train_accuracy, loss_print))\n",
    "        sess.run(optimizer, feed_dict={tf_X: batch[0], tf_Y: batch[1], keep_prob: 0.8})\n",
    "\n",
    "    test_accuracy = 0.0  \n",
    "    for i in range(10):\n",
    "        test_batch = next_batch(1000, x_test, y_test_one_hot.eval())\n",
    "        test_accuracy = test_accuracy + accuracy.eval(feed_dict={tf_X: test_batch[0], tf_Y: test_batch[1], keep_prob: 1.0})\n",
    "    test_accuracy = test_accuracy / 10;\n",
    "    print(\"Test accuracy: %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "되게 오래 걸린다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
